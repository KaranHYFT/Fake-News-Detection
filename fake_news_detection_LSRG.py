# -*- coding: utf-8 -*-
"""Fake_news_detection_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wZ4Sx5LQFgt475Zb11qqdlykE4m45WyH

# Natural Language Processing
## Fake News Detection Using Random Forest, logistic regression, SVM and Naive Bayes.
"""

!pip3 install pytest-runner --upgrade
!pip3 install ftfy

# Commented out IPython magic to ensure Python compatibility.
import ftfy
import nltk
from nltk.stem import SnowballStemmer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
nltk.download('stopwords')
nltk.download('punkt')
import seaborn as sns
import pandas as pd
import csv
import numpy as np
import warnings
import json
import re
# %matplotlib inline
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB,GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer,TfidfVectorizer
from sklearn.preprocessing import Binarizer
from collections import Counter
from os import listdir, makedirs
from os.path import isfile, join, splitext, split
from wordcloud import STOPWORDS, WordCloud
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.decomposition import TruncatedSVD
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn import model_selection, svm
from sklearn.base import BaseEstimator, TransformerMixin
warnings.filterwarnings('ignore')
np.random.seed(0)

"""# Loading the LIAR Dataset

---
The data is categorised into four tab-separated files. It's being loaded as a pandas dataframe. Creating a single file from the train, test, and validation files so that we can conduct the train and test split as needed.

#Mounting the dataset through Google drive
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Importing the dataset
train = pd.read_csv('/content/gdrive/MyDrive/project/enter/data/train.tsv', delimiter='\t', quoting=3, header=None)
test = pd.read_csv('/content/gdrive/MyDrive/project/enter/data/test.tsv', delimiter='\t', quoting=3, header=None)
valid = pd.read_csv('/content/gdrive/MyDrive/project/enter/data/valid.tsv', delimiter='\t', quoting=3, header=None)

raw_dataframe = pd.concat([train, test, valid], axis=0, sort=False)
raw_dataframe = raw_dataframe.sample(frac=1).reset_index()
print(raw_dataframe.shape)
raw_dataframe.columns=["index","ID", "label", "statement", "subject", "speaker", "job", "state", "party", "barely_true_cts",
        "false_cts", "half_true_cts", "mostly_true_cts", "pants_on_fire_cts", "context"]
raw_dataframe.head(5)

"""#printing the first 15 statement by speaker in list form the csv file"""

# some of the statements
raw_dataframe['statement'].head(15).tolist()



"""# Exploratory Data Analysis for to check the statement according to dataset

"""

sns.countplot(x='label',data=raw_dataframe)
plt.title('statement type and number of classes')

"""### Converting multiclass labels to two classes - Fake and True for exploratory data analysis

#function for mapping labels "true, mostly-true, half-true" to TRUE and "false, barely-true, pants-fire" to FAKE.
"""

def binary_class_dataset(input):
    
    input = input.iloc[:, [2, 3]]
    input.columns = ['label', 'statement']
    Original_labels = {
        'true': 'True',
        'mostly-true': 'True',
        'half-true': 'True',
        'false': 'False',
        'barely-true': 'False',
        'pants-fire': 'False'
    }
    input['label'] = input['label'].map(Original_labels)

    return input

"""# running the function on the loaded dataframe
#and converting into newdataframe to use in the different models
"""

new_df= binary_class_dataset(raw_dataframe)

"""## preparing Word Clouds for the text statements in the dataset
# creating two dataset consisting of True and Fake statements seperately
"""

np.random.seed(0)
true_statements_dataset = new_df[new_df['label'].astype(str) == 'True']
fake_statements_dataset = new_df[new_df['label'].astype(str) == 'False']

"""### Exploring the 'speakers' who have given the statements in the LIAR dataset
#finding the total number of unique 'speakers' in the LIAR
"""

speakers_info = raw_dataframe.copy()
speakers = speakers_info['speaker'].unique()

print("Number of speakers in the Data is : %d  " % len(speakers))
print("Some top 10 speakers are:\n", speakers[:10].tolist())

"""#displaying all the speakers in exploratory plot will not be possible nor meaningful. Hence finding the top speakers who have made frequent statements"""

speakers_cts = speakers_info.groupby("speaker").statement.count()
# speakers who have made more than 100 statements.
speakers = speakers_cts[speakers_cts > 100] 
print("The speaker have made  %d statements more than 50 times in the dataset" % len(speakers))

print("Some of the speakers and the number of statements made by them are:\n", speakers[:10])

"""#Limiting ourselves to only prominent speakers for visualization"""

speakers_info = speakers_info[speakers_info['speaker'].isin(speakers.keys())]

"""#function to groupby the speakers by the class and count of statements spoken individuals"""

def class_counts(input, column):
    return input.groupby([column,"label"]).agg({"ID" : "count"})

"""#### Plot of top 13 speakers in the LIAR dataset labelled by class and count of statements they have given."""

# stacked bar plot of speakrs with class and count of statements
breakdown = class_counts(speakers_info, 'speaker')
breakdown.unstack().plot(kind='bar', stacked=True, figsize=(15,5))
plt.title("Top 13 speakers in the LIAR dataset with count and class of statements made")

"""# Normalising the statements and the speakers for comparison purpose

"""

def normalize(input, column):
    norm = input.reset_index()
    norm = norm.set_index([column,'label'])['ID'].unstack(fill_value=0)

    row_totals = norm.sum(axis=1)
    for row in range(len(norm.index)):
        norm.iloc[row]/= row_totals[row]

    norm = norm.stack().reset_index(name='Percent').set_index([column, 'label'])
    return norm

"""#### Normalised plot of top 13 speakers in the LIAR dataset labelled type of statements they have given (for comparison)"""

unifrom_data = normalize(breakdown, 'speaker')
unifrom_data.unstack().plot(kind='bar', stacked=True, figsize=(15,5))
plt.title("Top 13 speakers in the LIAR dataset with count and class of statements made by them")

"""In comparison to other speakers, Facebook postings, Chain emails, and Donald Trump have a larger number of 'pant on fire' false claims. We should also observe that, despite making the most remarks, Obama's pant-on-fire utterances are practically non-existent.

# Using CountVectorizer for Extracting features from 'statements' in LIAR dataset
"""

# specifying features and labels
X_data= raw_dataframe['statement']
y_data=raw_dataframe['label']

"""# specifying train and test split with ratio of 70:30"""

X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size=0.3, stratify=y_data)

"""###creating Custom Pre - Processing and Tokenisation of the data


---


# Regular expression for cleaning the statements


---

#Preprocessing the text in the statements


---
#regular expression for custom tokenisation


---




"""

hashtag_removal = re.compile(r"#\w+")
mention_removal = re.compile(r"@\w+")
url_removal = re.compile(r"(?:https?://)?(?:[-\w]+\.)+[a-zA-Z]{2,9}[-\w/#~:;.?+=&%@~]*")
extras_removal = re.compile("[.;:!\'?,\"()\[\]]")

def preprocess(writeup):
    processing_text = hashtag_removal.sub("[hashtag]",writeup)
    processing_text = mention_removal.sub("[mention]",processing_text)
    processing_text = extras_removal.sub("",processing_text)
    processing_text = url_removal.sub("[url]",processing_text)
    processing_text = ftfy.fix_text(processing_text)
    return processing_text.lower()

"""#([]|words|other non-space)


---


# defining 3 types of tokenisation
"""

tokenise_removal = re.compile(r"(\[[^\]]+\]|[-'\w]+|[^\s\w\[']+)") 
def custom_tokenise(writeup):
    return tokenise_removal.findall(text.lower())

def Tokenizer(string_input):
    words = re.sub(r"[^A-Za-z0-9\-]", " ", string_input).lower().split()
    port_stemmer=nltk.PorterStemmer()
    words = [port_stemmer.stem(word) for word in words]
    return words

def nltk_twitter_tokenise(text):
    twitter_token = nltk.tokenize.TweetTokenizer()
    return twitter_token.tokenize(text.lower())

"""# stop words list set to english"""

tostop_list = stopwords.words('english') # stop word list

"""### Defining custom functions for displaying results of Classification"""

# function for results of cross-validation
def displaying_scores_summary(name, scores):
    print("{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))


# fucntion for results of model fitting
def result_display():
    print("Accuracy: ", accuracy_score(y_test, analysis_output))
    print(classification_report(y_test, analysis_output))
    print(confusion_matrix(y_test, analysis_output))

"""#### When documents are shorts as 'statements' in LIAR datasets, using Binarizer for normalisation instead of Tfidf Transformer (i.e. scale to 0 for not present or 1 for presence at any frequency) is beneficial."""

model = Pipeline([
    ('vectorizer', CountVectorizer(analyzer='word',preprocessor=preprocess,tokenizer=Tokenizer,stop_words=tostop_list)),
    ('norm', Binarizer()),
    ('clf', LogisticRegression(solver='liblinear', random_state=0)),
])
model.fit(X_train, y_train)
analysis_output = model.predict(X_test)
result_display()



# code to view the selected features
vectorizer = CountVectorizer(analyzer='word')
selector = SelectKBest(chi2, k=100)
feats = vectorizer.fit_transform(X_train)
filtered = selector.fit_transform(feats, y_train)

from itertools import compress
cols = selector.get_support()
names = vectorizer.get_feature_names()

print(list(compress(names,cols)))

# Using Multinomial Naive Bayes classifier
model.set_params(clf=MultinomialNB())
model.fit(X_train, y_train)
analysis_output = model.predict(X_test)
result_display()

# Fitting C-Support Vector Classifier
model.set_params(clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',random_state=1000))
model.fit(X_train, y_train)
analysis_output = model.predict(X_test)
result_display()

# fitting Random forest classifier
model.set_params(clf=RandomForestClassifier(random_state=1000))
model.fit(X_train, y_train)
analysis_output = model.predict(X_test)
result_display()

"""## Creating Pipeline with TFID Vectorizer along with Feature Union of 'total words' in the 'statements'

Term Frequency — Inverse Document is abbreviated as TF-IDF. Term Frequency is a measure of how frequently a term appears in a document. Words that appear often across documents are scaled using Inverse Document Frequency. TF-IDF highlights words that are more intriguing, such as those that appear often in one document but not in others.

However in this instance we use feature union as well by incorporating the total words in each sentence as another feature. We build a custom transformer with two classes for features in the statements and the total number of words in the statements. By setting min\_df to 0.25 and max\_df to 0.75 we restrict the terms in the vocabulary based on document frequency
"""

class TextSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.field]
class NumberSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[[self.field]]

raw_dataframe_tfid= raw_dataframe.copy()

header_list=["index","ID", "label", "statement","TotalWords","Lemmatised_words", "subject", "speaker", "job", "state", "party", "barely_true_cts",
        "false_cts", "half_true_cts", "mostly_true_cts", "pants_on_fire_cts", "context"]
raw_dataframe_tfid = raw_dataframe_tfid.reindex(columns = header_list)

"""# creating new column to hold total number of words in the statements and calculating the total words"""

raw_dataframe_tfid['TotalWords'] = raw_dataframe_tfid['statement'].str.split().str.len()

X = raw_dataframe_tfid[['statement', 'TotalWords']]

Y = raw_dataframe_tfid['label']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

"""## Creating the pipeline with TFIDVectorizer and feature union of total words in statements"""

classifier = Pipeline([
    ('features', FeatureUnion([
        ('text', Pipeline([
            ('colext', TextSelector('statement')),
            ('tfidf', TfidfVectorizer(analyzer='word',preprocessor=preprocess, tokenizer=Tokenizer, stop_words=tostop_list,
                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB
        ])),
        ('words', Pipeline([
            ('wordext', NumberSelector('TotalWords')),
            ('wscaler', StandardScaler()),
        ])),
    ])),
    ('clf', LogisticRegression(solver='liblinear', random_state=0)),
    ])

"""# logistic regression"""

classifier.fit(X_train, y_train)
analysis_output = classifier.predict(X_test)
result_display()

"""#multinominal naive bayes fit and using gaussianNB"""

classifier.set_params(clf=GaussianNB())
classifier.fit(X_train, y_train)
analysis_output = classifier.predict(X_test)
result_display()

"""#SVM Support vector machine"""

classifier.set_params(clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',random_state=1000))
classifier.fit(X_train, y_train)
analysis_output = classifier.predict(X_test)
result_display()

"""#Random Forest"""

classifier.set_params(clf=RandomForestClassifier(random_state=1000))
classifier.fit(X_train, y_train)
analysis_output = classifier.predict(X_test)
result_display()



"""## Using char_wb for the TFIDVectorizer and features union"""

classifier_char_wb = Pipeline([
    ('features', FeatureUnion([
        ('text', Pipeline([
            ('colext', TextSelector('statement')),
            ('tfidf', TfidfVectorizer(analyzer='char_wb',preprocessor=preprocess, tokenizer=Tokenizer, stop_words=tostop_list,
                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB
        ])),
        ('words', Pipeline([
            ('wordext', NumberSelector('TotalWords')),
            ('wscaler', StandardScaler()),
        ])),
    ])),
    ('clf', LogisticRegression(solver='liblinear', random_state=1000)),
    ])

"""#logistic regression"""

classifier_char_wb.fit(X_train, y_train)
analysis_output = classifier_char_wb.predict(X_test)
result_display()

"""# Gaussian NB"""

classifier_char_wb.set_params(clf=GaussianNB())
classifier_char_wb.fit(X_train, y_train)
analysis_output = classifier_char_wb.predict(X_test)
result_display()

"""#Support vector Machine classifier"""

classifier_char_wb.set_params(clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))
classifier_char_wb.fit(X_train, y_train)
analysis_output = classifier_char_wb.predict(X_test)
result_display()

"""#Random Forest """

classifier_char_wb.set_params(clf=RandomForestClassifier())
classifier_char_wb.fit(X_train, y_train)
analysis_output = classifier_char_wb.predict(X_test)
result_display()

"""## Lemmatisation of the 'statements' by giving POS (parts of speech) Tags as context

The NLTK porter stemmer was previously used to stem the'statements' in the LIAR dataset. We now use stemming. Both are aiming towards the same result: to reduce each word's inflectional forms to a single base or root. The difference between lemmatization and stemming is that a stemmer works on a single word without understanding the context, whereas lemmatization works on several words. As a result, it is impossible to distinguish between words that have multiple meanings based on their position in the sentence. The context for lemmatization is provided here via POS (parts of speech tagging). Stemmers are often easier to use and run faster, but their accuracy suffers.
"""

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""#WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun. Here we consider only POS tags of Noun, Adjective, Verb and Adverb."""

corpus=[]

for state in raw_dataframe_tfid['statement']:
    
    texts=preprocess(state)
    token=nltk.word_tokenize(texts)
    corpus.append(token)

tag_map = defaultdict(lambda : wn.NOUN)
tag_map['J'] = wn.ADJ
tag_map['V'] = wn.VERB
tag_map['R'] = wn.ADV

for index,entry in enumerate(corpus):
    # looping through the entries and saving in the corpus
    Final_words = []
    # fitting WordNetLemmatizer()
    word_Lemmatized = WordNetLemmatizer()
    # pos_tag will provide the 'tag' i.e if the word is Noun(N) or Verb(V) etc.
    for word, tag in pos_tag(entry):
        # condition is to check for Stop words and consider only alphabets
        if word not in stopwords.words('english') and word.isalpha():
            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
    # The processed words for each 'statement' will be store in column 'lemmatised_words in the dataframe'
    raw_dataframe_tfid.loc[index,'Lemmatised_words'] = str(Final_words)

"""# sample lemmatised text of the 'statements'"""

corpus[1:2]

X=raw_dataframe_tfid['Lemmatised_words'].tolist()
y=raw_dataframe_tfid['label']

"""# fitting TfidfVectorizer with the lemmatised 'statements' using labelencoder"""

Encoder = LabelEncoder()
y = Encoder.fit_transform(y)

Tfidf_vect = TfidfVectorizer()
Tfidf_vect.fit(raw_dataframe_tfid['Lemmatised_words'])
X = Tfidf_vect.transform(X)

"""# logistic regression classifier"""

logistic = LogisticRegression(solver='liblinear', random_state=0)

"""# cross validation of logistic with stratifiedkfold """

cv_scores = cross_validate(logistic, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])
displaying_scores_summary("Accuracy", cv_scores['test_accuracy'])
displaying_scores_summary("Precision", cv_scores['test_precision_weighted'])
displaying_scores_summary("Recall", cv_scores['test_recall_weighted'])
displaying_scores_summary("F1", cv_scores['test_f1_weighted'])

"""#SVM """

SVM_classifier=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=0)

"""#Cross Validate SVM and Stratifiedkfold"""

cv_scores = cross_validate(SVM_classifier, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])
displaying_scores_summary("Accuracy", cv_scores['test_accuracy'])
displaying_scores_summary("Precision", cv_scores['test_precision_weighted'])
displaying_scores_summary("Recall", cv_scores['test_recall_weighted'])
displaying_scores_summary("F1", cv_scores['test_f1_weighted'])





"""## Converting the Multiclass labels into Binary class labels (Fake & True) and predicting"""

new_df.head(5)

new_df['TotalWords'] = new_df['statement'].str.split().str.len()

new_df.head(2)

X = new_df[['statement', 'TotalWords']]

Y=  new_df['label']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

classifier_biclass = Pipeline([
    ('features', FeatureUnion([
        ('text', Pipeline([
            ('colext', TextSelector('statement')),
            ('tfidf', TfidfVectorizer(analyzer='word',preprocessor=preprocess, tokenizer=Tokenizer, stop_words=tostop_list,
                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),
            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB
        ])),
        ('words', Pipeline([
            ('wordext', NumberSelector('TotalWords')),
            ('wscaler', StandardScaler()),
        ])),
    ])),
    ('clf', LogisticRegression(solver='liblinear', random_state=0)),
    ])

classifier_biclass.fit(X_train, y_train)
analysis_output = classifier_biclass.predict(X_test)
result_display()

matrix=confusion_matrix(y_test, analysis_output) # getting the results of confusion matrix from the classification.
sns.heatmap(matrix, annot = True,fmt='g')       # printing the matrix
plt.title('Logistic Regression binary class Confusion-Matrix')
plt.ylabel('True labels')
plt.xlabel('Predicted labels');

"""#GaussionNB"""

classifier_biclass.set_params(clf=GaussianNB())
classifier_biclass.fit(X_train, y_train)
analysis_output = classifier_biclass.predict(X_test)
result_display()

"""#SVM Support vector machine"""

classifier.set_params(clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))
classifier_biclass.fit(X_train, y_train)
analysis_output = classifier_biclass.predict(X_test)
result_display()

"""#Decision Tress classifier"""

decision_tree=tree.DecisionTreeClassifier(random_state=1000)
classifier.set_params(clf=decision_tree)
classifier_biclass.fit(X_train, y_train)
analysis_output = classifier_biclass.predict(X_test)
result_display()

"""#Random Forest"""

classifier.set_params(clf=RandomForestClassifier())
classifier_biclass.fit(X_train, y_train)
analysis_output = classifier_biclass.predict(X_test)
result_display()